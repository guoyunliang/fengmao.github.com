---
title:   [Tair源代码分析<2>]
layout: post
category: tair
tags: memory kernel
---
<script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js?lang=cc&skin=sons-of-obsidian"></script>
###写在最前面
重新调整了下章节，对部分内容作了调整。
##2.5 重建对照表
在 TAIR源码分析<一>中，介绍了configserver初始化后的工作状况。本文将详细介绍configserver如何创建对照表。
###2.5.1什么是对照表
在Tair系统中，对照表是一个非常重要的概念。Tair使用改进后的一致性hash算法来确保数据在数据服务器上趋于均匀分布，同时获取良好的可扩展性。Tair将数据按bucket为单位，存放在存储节点上，每一个节点可以存储多个bucket。对于每一个bucket而言，可能会存在多个备份。若在多备份的情况下，其中一个bucket称为master bucket,其他的称为slave bucket。Tair客户端会从configserver获取到一张表，该表记录了每一个bucket存储的节点的地址。这张表，我们称之为对照表。

那么，对于客户端而言，拿到对照表后，如何通过对照表来写或读一个<key,value>呢？具体地，客户端作hash(key)%count_of_bucket运算，获取具体的bucket id。通过表，查找backet所在的存储节点地址。接下来的事情，就是客户端将这个KV存储到这个地址对应的节点中对应的bucket中。而这个表，就称为对照表。

###2.5.2对照表逻辑结构及其作用
在叙述对照表构造算法前，有必要对对照表的逻辑结构有一个大概介绍。对照表本质上是一个映射表。其基本功能是描述bucket与其存储节点标示符的映射。

为什么会重建对照表呢？虽然key到bucket之间的映射是固定的。但是, bucket到存储节点之间的映射是无法固定的。具体原因有: 1）节点不可靠；2）扩容（缩容）。正因为，这个映射是无法固定的。当出现上述2中情况中的任何一种，就需要重新维护bucket到存储节点的映射关系，即重建对照表。

考虑到数据安全性（在系统中存储多份相同数据）和单个节点失效仍然提供有限服务等因素，configserver维护3个对照表。分别记为hash_table, m_hash_table, d_hash_table。这3个对照表逻辑结构都是相同的，即由copyCount(该项可配置）行构成，bucketCount列。考虑数据安全性，tair将数据备份为copyCount份。为了方便叙述，我这些内容相同的bucket分为2类——master bucket和slave bucket。对照表的第0行，存储的是master bucket的标示符，第1到copycount-1行存储的是slave bucket的标示符。假设，有4个存储节点，分别为A,B.C和D，copyCount为3。系统在启动之初，建立的对照表可能（有随机因素加入，每一次启动不一定都生成一样的对照表）如下所示：

<p align=center><img src=/images/tair-2/t1.png width=600></p>

如表1所示，共有3个对照表，从左到右边，依次为hash_table, m_hash_table, d_hash_table。通常情况下，这些对照表的内容相同的。只有在对照表重建后，并且有限的时间内，这些表内容不同。当重建对照表后，系统会触发数据迁移工作。待数据迁移完成后，这3张对照表内容就相同了。也就是说，只有在数据迁移过程中，对照表的内容才会不同。
 
对照表创建后，configserver会将hashtable表发送给客户端，客户端通过这个对照表，依然可以读写数据，不至于在对照表重建过程中，无法提供服务。Configserver会将m_hash_table和d_hash_table发送到dataserver, dataserver通过这两个表来确定2件事：1）自己负责存储的bucket id；2)确定迁移的bucket id以及迁移的目标机器。关于bucket迁移后续在讨论。当数据迁移完成后。这个3个对照表又会变成相同的内容了。
###2.5.3对照表重建算法
我们可以考察该算法的输入和输出来考察该算法。该算法的输入为m_hash_table对照表,和当前可用节点列表。输出hash_table, m_hash_table, d_hash_table。

在tair的配置文件中，有对照表创建策略的配置项，即_build_strategy。configserver根据_build_strategy的取值不同，而执行不同的创建对照表策略。目前提供两种策略.配置为1则是负载均衡优先,分配的时候尽量让各个data server的负载均衡.配置为2的时候,是位置安全优先,会尽量将一份数据的不同备份分配到不同机架的机器上.配置为3的时候，如果服务器分布在多个机器上，那么会优先使用位置安全优先，即策略2.如果服务器只在一个机架上，那么退化成策略1，只按负载分布[1].

实际上，Tair提供了2中构建对照表的方法。对于每一种方法，构建对照表的流程是相同的。对于不同构建策略，每一个节点上分配的bucket数量的分配方式是不同的。这一点很容易理解。另外，对于一个给定的buket,为其选择一个合适的存储节点的策略也是不同的。例如，考虑位置安全优先的对照表构建策略中，buket和其备份bucket应该尽可能分布在不同机房。

接下来，就具体分析在每一种策略下，重建对照表的过程。
####2.5.3.1负载均衡优先策略
在该策略下，构建出的对照表期望达到的效果——使得每一个节点分配的bucket数量尽可能的均衡。假设共有B个bucket,有N个节点，那么在负载均衡优先的策略下，每个节点最少负责的bucket的个数为：B/N;若B%N不为0，那么将有B%N个节点将负载B/N+1个bucket。若B%N为0，那就再好不过了，每个节点分配了相同数量的bucket。由此可见，在这种模式下，任意两个节点负责的bucket数量至多相差1（这只是期望值）。具体地，负载均衡策略构建对照表大致步骤如下：

__1)__ 根据当前m_hash_table表，统计出每一个alive节点上存储的bucket数量。假设有节点A，B，C和D，则形成数据如下的map:

![alt table](/images/tair-2/t2.png "map")

其中Xa表示节点A上存储的bucket数量，Xb,Xc,Xd亦然。显然，在启动时Xa = Xb = Xc = Xd = 0;

__2)__ 根据1）中生成的map构建一个index map (实在不知道如何称呼了），即对于hold相同数量bucket的节点存放在同一列表里，举例如下：

![alt table](/images/tair-2/t3.png "map")

若在1）中Xa == Xb == n1，那么形成一个列表[A, B];

__3)__  给每一个节点分配存储bucket的数量。这一步只是确定每一个节点上可以储存多少个bucket,而不确定到底存储哪一些bucket。对于不同的构建对照表策略，这一步骤是不同的。
在具体给每个节点分配bucket的过程中，假设某个节点上已经分配了b个bucket,现在N个节点已经负责了X个bucket,若b < X/N，则让该节点负责B/N个bucket,否则该节点负责B/N+1个节点。这样做的目的是，在负载均衡的前提下，减少数据迁移。由于允许数据存放多备份，对于某个bucket而言，可能会存储copyCount次，也就是说存在copyCount个bucket的内容是完全一样的，这些一样的backet，我们将其中的一个称作为master bucket,其余的称为slave bucket。那对于master bucket显然不能存放在过于集中，也需平均分配在每一个节点上。也类似地构造一个map,如下：

![alt table](/images/tair-2/t4.png "map")

其中，Ca表示节点A最终要存储的bucket总数；这个步骤代码实现如下:

![alt code graph](/images/tair-2/caculate_capable.png "caculate_capable")

__4)__  计算每一个节点预期可存储bucket的数量；有了1) ~ 3)步的数据，现在可以计算每一个节点还可以存储的bucket的数量（实现中用负数的绝对值表示）。

__5)__  快速创建对照表。在这个步骤中，系统将hash_table的第0行中失效了的存储节点标示符替换掉。既然是替换，那从哪里来呢？将对应bucket的slave bucket的存储标示符存储到第0行相应位置（slave bucket选为master bucket）。这个替换时候，有copyCount-1个slave bucket可选，选择的存储节点上负载的master bucket没有超标，并且总bucket数没有超标，考虑到不同的构建策略，这里还有其他约束。替换后，将slave bucket对应的存储节点置0.这个过程是很快速的，只考虑对照表的第0行，O(N)时间完成。称为快速创建对照表。这步骤生成的表作为新对照表的hast_table和m_hash_table对照表的内容。

__6)__  确定每一个bucket的存储位置。

__首先__，为master bucket安排存储节点。对于每一个master bucket当前所在的节点X，检查X节点是否满足要求（每一种策略，需要满足的要求也不相同,后文叙述）。若不满足要求，则在该bucket的copyCount -1个slave bucket中选择一个bucket,将该bucket作为master bucket（这里有可能选择不到，即时存在slave bucket,也无法选择。因为，要保证节点上的总master bucket的数量不要超过一个值);若没有找到，则需要在alive的节点列表中，选择一个合适的节点S。将S节点作为这个master bucket的存储节点。更新统计数据。

__其次__，为slave bucket安排存储节点。具体步骤为，首先检查bucket当前的存储节点是否满足要求（若该节点down掉了，显然不满足要求了）。若不满足要求，为其选择一个合适的节点，将该节点作为bucket的存储节点。更新统计数据。这个步骤生成的对照表，作为d_hash_table的内容。

上述6）个步骤代码如下，代码中有注释：
<pre class="prettyprint cc">
//return value 0 build error  1 ok 2 quick build ok
int table_builder::rebuild_table(const hash_table_type &amp; hash_table_source,
    hash_table_type &amp; hash_table_result, bool no_quick_table)
{

   //步骤1） 
  init_token_count(tokens_count_in_node);
  init_token_count(tokens_count_in_node_now);
  init_token_count(mtokens_count_in_node);
  max_count_now = 0;
  bool need_build_quick_table = false;

  // compute node count: tokens_count_in_node, mtokens_count_in_node
  for(hash_table_type::const_iterator it = hash_table_source.begin();
      it != hash_table_source.end(); it++) {
    const hash_table_line_type &amp; line = it-&gt;second;
    for(uint32_t i = 0; i &lt; bucket_count; i++) {
      update_node_count(line[i], tokens_count_in_node);
      log_debug("line[%d]: %d, server: %s, count: %d",
          i, it-&gt;first, tbsys::CNetUtil::addrToString(line[i].first).c_str(), line[i].second);
      if(it-&gt;first == 0) {
        if(update_node_count(line[i], mtokens_count_in_node) == false) {
          // almost every time this will happen
          need_build_quick_table = true;
        }   
      }   
    }   
  }

  //步骤2）
  //init count_server, mcount_server
  build_index(tokens_count_in_node, count_server);
  build_index(mtokens_count_in_node, mcount_server);

  //步骤3）
  caculate_capable();

  //步骤4）
  //init mcandidate_node, scandidate_node
  init_candidate(mcandidate_node, &amp;master_server_capable, &amp;mcount_server);
  init_candidate(scandidate_node, &amp;server_capable, &amp;count_server);
  hash_table_result = hash_table_source;
  // no_quick_table is always true, will not reach this branch
  if(need_build_quick_table &amp;&amp; !no_quick_table) {
    if(build_quick_table(hash_table_result)) {
      return BUILD_QUICK;
    }   
    log_error("build quick table fail");
    return BUILD_ERROR;
  }
  if(available_server.size() &lt; copy_count) {
    log_error("rebuild table fail, available size: %u, copy count: %u", available_server.size(), copy_count);
    return BUILD_ERROR;
  }
  //
  //we will check master first, then other slaves
  /////////////////////////////////////////////////////////////////////////////////////
  //checke every node and find out the bad one
  //a good one must
  //    1 the buckets this one charge of must not large than tokenPerNode ;
  //    2 the master buckets this one charge of must not large than masterTokenPerNode;
  //    3 copys of a same bucket must seperated store in different data server
  /////////////////////////////////////////////////////////////////////////////////////
  int i = 0;

  //以下是步骤5）
  for(hash_table_type::iterator it = hash_table_result.begin();
      it != hash_table_result.end() &amp;&amp; i &lt; 2; it++, i++) {
    int line_num_out = it-&gt;first;
    for(uint32_t node_idx = 0; node_idx != bucket_count; node_idx++) {
      for(uint32_t line_num = line_num_out; line_num &lt; copy_count;
          line_num++) {
        if(line_num_out == 0 &amp;&amp; line_num != 0)
          continue;
        int change_type = 0;        //not need migrate
        server_id_type node_id = hash_table_result[line_num][node_idx];
        int consider = CONSIDER_ALL;
        if(line_num_out == 0)
          consider = CONSIDER_BASE;
        change_type =
          is_this_node_OK(node_id, line_num, node_idx, hash_table_result,
              consider, true);
        log_debug("change type: %d", change_type);
        if(change_type == INVALID_NODE) {
          node_id.first = INVALID_FLAG;
        }

        if(change_type != 0) {
          if(line_num == 0) {
            if(change_master_node(node_idx, hash_table_result, false)) {
              continue;
            }
          }
          server_id_type old_node = hash_table_result[line_num][node_idx];
          invaliad_node(line_num, node_idx, hash_table_result);
          server_id_type suitable_node =
            get_suitable_node(line_num, node_idx, hash_table_result,
                old_node);
          if(suitable_node.first == INVALID_FLAG) {
            log_error("I am give up, why this happend?\n");
            return BUILD_ERROR;
          }
          update_node(line_num, node_idx, suitable_node,
              hash_table_result);
          node_id = suitable_node;
        }
        int token_per_node_min = get_tokens_per_node(node_id);
        if(++tokens_count_in_node_now[node_id] == token_per_node_min + 1) {
          max_count_now++;
        }
        log_debug("token_per_node_min: %d, max_count_now: %d, tokens_count_in_node_now[%s]: %d\n",
            token_per_node_min, max_count_now,
            tbsys::CNetUtil::addrToString(node_id.first).c_str(), tokens_count_in_node_now[node_id]);
      }
    }
    if(it == hash_table_result.begin()) {
      log_debug("first line ok");
    }
  }
  return BUILD_OK;
}
</pre>

__7)__ 对照表保存和同步

__首先__，configserver会将d_hash_table的内容保存到本地磁盘上。

__其次__，根据配置信息（是否允许数据迁移，是否允许数据丢失）。若无需数据迁移，那么用d_hash_table替换m_hash_table, hash_table。若需要数据迁移，统计哪些节点需要数据迁移。并且，将m_hash_table, hash_table保存在本地磁盘。最后，将对照表同步到备configserver上。

经过上述7个步骤，对照表就创建完成了。若存在数据迁移，这3个对照表内容是不同的。随着迁移的完成，3张对照表内容会变成相同的。后文叙述这部分内容。
 
在构建对照表的过程中，有几个细节需要注意。其中，包括：1）检查可用节点列表；2）给每个存储节点分配bucket数量；3）判断某个bucket是否可存储在某个存储节点。实际上，2中对照表构建策略的不同之处也通过上述3个方面提现出来的。接下来，就分别描述下这几个细节。

__检查可用节点列表__

在负载优先策略下，对可用节点列表的约束是，需保证列表中的每一个节点都是alive的。

__计算某存储节点负载bucket的数量__

该部分内容已在上文介绍过了，具体请参见对照表构建步骤的第3）步。

__判断某个bucket是否适合存储在某个节点__

该功能由get_suitable_node函数实现，并且该函数通过调用is_this_node_ok函数。将在这里展开叙述get_suitable_node函数。该函数正如其名，其功能是选择一个合适的节点。虽然，configserver提供2中不同的建表策略，但是选择合适节点的框架是相同的。主要不同点在于，判断某一个节点是否适合存储某个特定bucket的约束是不同的，显然在不同的创建策略下，约束是不同的。这里先考察在负载均衡策略下，判断一个节点是否为适合存储某个bucket的约束归纳如下：

__1)__ 该节点上存储master bucket的数量约束；

__2)__ 该节点上存储总的bucket数量约束；

__3)__ 拥有最多bucket的节点总数量约束；

__4)__ 欲存储的bucket与其备份bucket不能在同一个节点上；

__5)__ 欲存储的bucket与其备份bucket不能在同一个机房中。

而选择合适节点采用的约束级别分为4类，根据其约束强弱，依次定义为ALL,POS,BASE和FORCE。

<table border="1" cellspacing="0" cellpadding="0" width="903" style="width:722px;line-height:20px">
	<tbody>
		<tr>
			<td>
				<p align="left">
					<span style="font-size: 14px">约束级别</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">ALL</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">POS</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">BASE</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">FORCE</span>
				</p>
			</td>
		</tr>
		<tr>
			<td>
				<p align="left">
					<span style="font-size: 14px">为master bucket选择节点满足约束</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">1）2）4）5）</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">1）3）4）5）</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">1）3）5）</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">1）</span>
				</p>
			</td>
		</tr>
		<tr>
			<td>
				<p align="left">
					<span style="font-size: 14px">为slave bucket选择节点满足约束</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">2）4）5）</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">2）4）5）</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">2）4）</span>
				</p>
			</td>
			<td>
				<p align="left">
					<span style="font-size: 14px">无</span>
				</p>
			</td>
		</tr>
	</tbody>
</table>
通过表中，我们可以看到一个有趣的现象同样的约束级别FORCE，当未master bucket选择存储节点时候，需要满足约束1），而为slave bucket选择节点的时候，无需满足任何约束。这一点需要注意的。

有了上述介绍，我们就开始介绍描述get_suitable_node函数的工作方式吧。实际上，该函数异常降低约束级别来选择合适的节点。例如，在ALL级别下，无法选择合适的节点，那么就将约束级别降低到POS，直到降低到FORCE级别。在每一级别的约束下，依次遍历一个节点存储bucket容量的map(该map在对照表构建过程的第2步创建），选择一个节点。这里需要注意的是，为了避免在容灾的双机房中，构造出”同构“的对照表，这里选择节点的过程中添加了随机因子。

####2.5.3.2位置优先策略构建对照表
所谓位置优先策略构建对照表是指，在构建对照表的时候，位置安全优先考虑, 会尽量将一份数据的不同备份分配到不同机架的机器上。这里需要注意的是，尽可能第达成这个目标。在该策略下，构建对照表的过程与2.5.3.1中描述的负载均衡优先构造对照表是一一样的。不同之处在于：

__1)__ 检查可用dataserver列表更加严格。

__2)__ 计算每一个节点负载的bucket数量的算法不同；

__3)__ 判断某个bucket是否适合存储在某个节点的算法不同；

接下来的章节，我将依次描述上述3个方面的不同之处。

__检查可用节点列表__
在负载均衡优先策略中，group会统计当前可用节点列表，将该列表与m_hash_table一起作为算法的输入。这里对可用节点列表的唯一约束就是，列表中的节点都是alive的。而在位置优先策略中，对可用节点列表的约束加强了，即除了要求列表中的每个节点是alive的外，还需要统计2个机房中dataserver的数量差异不超过一个阈值。若不满足这个2个约束，在该策略下，对照表是无法正常构建的。请看代码：
<pre class="prettyprint cc">
void table_builder2::set_available_server(const set &lt;
                                              node_info * &gt;&amp;ava_server)
{
  available_server.clear();
  map&lt;uint32_t, int&gt;pos_count;
  for(set&lt;node_info *&gt;::const_iterator it = ava_server.begin();
      it != ava_server.end(); it++) {
    log_info("mask %"PRI64_PREFIX"u:%s &amp; %"PRI64_PREFIX"u --&gt;%"PRI64_PREFIX"u",
        (*it)-&gt;server-&gt;server_id, tbsys::CNetUtil::addrToString((*it)-&gt;server-&gt;server_id).c_str(),
        pos_mask, (*it)-&gt;server-&gt;server_id &amp; pos_mask);
    available_server.
      insert(make_pair
          ((*it)-&gt;server-&gt;server_id,
           (*it)-&gt;server-&gt;server_id &amp; pos_mask));
    (pos_count[(*it)-&gt;server-&gt;server_id &amp; pos_mask])++;
  }
  pos_max = 0;
  for(map&lt;uint32_t, int&gt;::iterator it = pos_count.begin();
      it != pos_count.end(); ++it) {
    log_info("pos:%d count:%d", it-&gt;first, it-&gt;second);
    if(it-&gt;second &gt; pos_max) {
      pos_max = it-&gt;second;
      max_machine_room_id = it-&gt;first;
    }   
  }
 //这里检查两个机房的可用存储节点数的差异
  int diff_server = available_server.size() - pos_max - pos_max;
  if(diff_server &lt;= 0)
    diff_server = 0 - diff_server;
  float ratio = diff_server / (float) pos_max;
  if(ratio &gt; stat_change_ratio || available_server.size() &lt; copy_count
      || ratio &gt; 0.9999) {
    build_stat_normal = false;
  }
  else {
    build_stat_normal = true;
  }
  log_warn("diff_server = %d ratio = %f stat_change_ratio=%f",
      diff_server, ratio, stat_change_ratio);
}
</pre>

__计算每一个存储节点负载的bucket数量__

在位置安全优先策略中，根据配置掩码pos_mask来将节点分成2个机房，并且统计出每个机房中可用的存储节点数量。当确定出2个机房后，依次确定每个机房中的存储节点的平均负载bucket的数量（记为B), 拥有B个bucket的节点数量，拥有B+1个bucket的节点数量。接下来，为存储节点分配bucket数量，会根据其所在节点的平均bucket数量（每个机房的这3个数据可能是不同的）等数据，确定该节点的负载的bucket数量。这里将代码贴上：
<pre class="prettyprint cc">
void table_builder2::caculate_capable()
{
  int available_server_count = available_server.size();
  int real_copy_count = copy_count;

  if(copy_count &lt;= 1) {
    build_stat_normal = false;
  }
  if(build_stat_normal == false) {
    if(real_copy_count &gt; 2) {
      real_copy_count = 2;
    }   

  }
  int mtoken_per_node_min;
  int mtoken_per_node_max_count;
  int mtoken_per_node_min_count;
  int otoken_per_node_min;
  int otoken_per_node_max_count;
  int otoken_per_node_min_count;
  int mmaster_token_per_node_min;
  int mmaster_token_per_node_max_count;
  int mmaster_token_per_node_min_count;
  int omaster_token_per_node_min;
  int omaster_token_per_node_max_count;
  int omaster_token_per_node_min_count;


  assert(build_stat_normal);
  int token_per_node_min =
    bucket_count * real_copy_count / available_server_count;
  int token_per_node_max_count =
    bucket_count * real_copy_count -
    token_per_node_min * available_server_count;
  int token_per_node_min_count =
    available_server_count - token_per_node_max_count;

  int master_token_per_node_min = bucket_count / available_server_count;
  int master_token_per_node_max_count =
    bucket_count - master_token_per_node_min * available_server_count;
  int master_token_per_node_min_count =
    available_server_count - master_token_per_node_max_count;

  if(build_stat_normal) {
    assert(real_copy_count &gt;= 2);

    int total_bucket = bucket_count * (real_copy_count - 1);
    int master_bucket =
      (int) (bucket_count * ((float) pos_max / available_server_count));
    int server_count = pos_max;

    int balance_bucket =
      (int) (bucket_count * real_copy_count *
          ((float) pos_max / available_server_count));

    if(total_bucket &gt; balance_bucket) {
      total_bucket = balance_bucket;
    }
    mtoken_per_node_min = total_bucket / server_count;
    mtoken_per_node_max_count =
      total_bucket - mtoken_per_node_min * server_count;
    mtoken_per_node_min_count = server_count - mtoken_per_node_max_count;
    //计算拥有最多存储节点的机房中的每个节点平均bucket数(记为B），拥有B+1个bucket的存储节点数，拥有B个bucket的存储节点数；
    mmaster_token_per_node_min = master_bucket / server_count;
    mmaster_token_per_node_max_count =
      master_bucket - mmaster_token_per_node_min * server_count;
    mmaster_token_per_node_min_count =
      server_count - mmaster_token_per_node_max_count;


    total_bucket = bucket_count * real_copy_count -
      (mtoken_per_node_min * mtoken_per_node_min_count +
       (mtoken_per_node_min + 1) * mtoken_per_node_max_count);
    master_bucket =
      bucket_count -
      (mmaster_token_per_node_min * mmaster_token_per_node_min_count +
       (mmaster_token_per_node_min +
        1) * mmaster_token_per_node_max_count);
    server_count = available_server_count - server_count;

    //计算另外一个机房的上述数据
    otoken_per_node_min = total_bucket / server_count;
    otoken_per_node_max_count =
      total_bucket - otoken_per_node_min * server_count;
    otoken_per_node_min_count = server_count - otoken_per_node_max_count;

    omaster_token_per_node_min = master_bucket / server_count;
    omaster_token_per_node_max_count =
      master_bucket - omaster_token_per_node_min * server_count;
    omaster_token_per_node_min_count =
      server_count - omaster_token_per_node_max_count;
  }

  server_capable.clear();
  //为拥有最多存储节点的那个机房中的每一个存储节点的capable.
  master_server_capable.clear();
  {
    token_per_node_min = mtoken_per_node_min;
    token_per_node_max_count = mtoken_per_node_max_count;
    token_per_node_min_count = mtoken_per_node_min_count;

    master_token_per_node_min = mmaster_token_per_node_min;
    master_token_per_node_max_count = mmaster_token_per_node_max_count;
    master_token_per_node_min_count = mmaster_token_per_node_min_count;

    log_info("node in max room");
    log_info("tokenPerNode_min=%d", token_per_node_min);
    log_info("tokenPerNode_max_count=%d", token_per_node_max_count);
    log_info("tokenPerNode_min_count=%d", token_per_node_min_count);

    log_info("masterTokenPerNode_min_count=%d",
        master_token_per_node_min_count);
    log_info("masterTokenPerNode_max_count=%d",
        master_token_per_node_max_count);
    log_info("masterTokenPerNode_min_count=%d",
        master_token_per_node_min_count);

    int max_s = 0;
    int mmax_s = 0;
    int i = 0;
    for(server_list_type::const_iterator it = available_server.begin();
        it != available_server.end(); it++) {
      if((*it).second != max_machine_room_id)
        continue;
      if(max_s &lt; token_per_node_max_count) {
        server_capable[*it] = token_per_node_min + 1;
        max_s++;
      }
      else {
        server_capable[*it] = token_per_node_min;
      }

      if(mmax_s &lt; master_token_per_node_max_count) {
        master_server_capable[*it] = master_token_per_node_min + 1;
        mmax_s++;
      }
      else {
        master_server_capable[*it] = master_token_per_node_min;
      }
      i++;
    }
  }
   //为另外一个机房中的每一个存储节点的capable.
  {
    token_per_node_min = otoken_per_node_min;
    token_per_node_max_count = otoken_per_node_max_count;
    token_per_node_min_count = otoken_per_node_min_count;

    master_token_per_node_min = omaster_token_per_node_min;
    master_token_per_node_max_count = omaster_token_per_node_max_count;
    master_token_per_node_min_count = omaster_token_per_node_min_count;

    log_info("node not in max room");
    log_info("tokenPerNode_min=%d", token_per_node_min);
    log_info("tokenPerNode_max_count=%d", token_per_node_max_count);
    log_info("tokenPerNode_min_count=%d", token_per_node_min_count);

    log_info("masterTokenPerNode_min_count=%d",
        master_token_per_node_min_count);
    log_info("masterTokenPerNode_max_count=%d",
        master_token_per_node_max_count);

    int max_s = 0;
    int mmax_s = 0;
    int i = 0;
    for(server_list_type::const_iterator it = available_server.begin();
        it != available_server.end(); it++) {
      if((*it).second == max_machine_room_id)
        continue;
      if(max_s &lt; token_per_node_max_count) {
        server_capable[*it] = token_per_node_min + 1;
        max_s++;
      }
      else {
        server_capable[*it] = token_per_node_min;
      }

      if(mmax_s &lt; master_token_per_node_max_count) {
        master_server_capable[*it] = master_token_per_node_min + 1;
        mmax_s++;
      }
      else {
        master_server_capable[*it] = master_token_per_node_min;
      }
      i++;
    }
  }
}
</pre>

__判断某个bucket是否适合存储在某个节点__

在负载均衡策略下，讨论了如何判断某个bucket是否适合存储在某个节点上。在位置优先策略下，也是通过通过各种约束条件来实现这一功能的。位置安全优先策略将所有节点按位置分为2类。与负载均衡策略所不同的是：首先，判断某个节点是否可以存储某个bucket的时候，需要考虑其所属位置类别的信息，而非负载均衡那种全局信息（例如，A机房中的节点n, 其存储bucket的数量是以该机房中所有节点的平均负载 bucket数量为基准的，而不是以所有机房中的为基准)；其次，每种约束级别需满足的约束条件不同；然后，没有添加随机扰动因子。现将这些约束归类如下：

__1)__ 在所在机房里，该节点上存储master bucket的数量约束；

__2)__ 在所在机房里，该节点上存储总的bucket数量约束；

__3)__ 在所在机房了，拥有最多bucket的节点总数量约束；

__4)__ 欲存储的bucket与其备份bucket不能在同一个节点上；

__5)__ 欲存储的bucket与其备份bucket不能在同一个机房中。
而选择合适节点采用的约束级别分为4类，根据其约束强弱，依次定义为ALL,POS,BASE和FORCE。

<table border="1" cellspacing="0" cellpadding="0" width="903" style="width: 722px; line-height: 20px; font-family: tahoma, Arial, 微软雅黑">
	<tbody>
		<tr>
			<td>
				<p align="left">
					约束级别
				</p>
			</td>
			<td>
				<p align="left">
					ALL
				</p>
			</td>
			<td>
				<p align="left">
					POS
				</p>
			</td>
			<td>
				<p align="left">
					BASE
				</p>
			</td>
			<td>
				<p align="left">
					FORCE
				</p>
			</td>
		</tr>
		<tr>
			<td>
				<p align="left">
					为master bucket选择节点满足约束
				</p>
			</td>
			<td>
				<p align="left">
					1）4）5）
				</p>
			</td>
			<td>
				<p align="left">
					1）4）5）
				</p>
			</td>
			<td>
				<p align="left">
					1）
				</p>
			</td>
			<td>
				<p align="left">
					1）
				</p>
			</td>
		</tr>
		<tr>
			<td>
				<p align="left">
					为slave bucket选择节点满足约束
				</p>
			</td>
			<td>
				<p align="left">
					2）4）5）
				</p>
			</td>
			<td>
				<p align="left">
					2）3）4）5）
				</p>
			</td>
			<td>
				<p align="left">
					2）3）4）5）
				</p>
			</td>
			<td>
				<p align="left">
					4）5）
				</p>
			</td>
		</tr>
	</tbody>
</table>
通过表中数据，我们可以看出负载均衡策略与位置安全策略在判断某bucket是否适合存储在某个节点上的判断策略还是很不同的。

###2.5.4 小结
2.5.*节主要介绍了对照表的构建。后续章节介绍bucket的迁移。
