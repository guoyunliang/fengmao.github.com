---
title: 由malloc函数引发的<s>内存"览胜"</s>思考
layout: post
tags: malloc glibc libc memory allocation kernel ptmalloc heap mmap
category: linux
---

##glibc

linux默认提供了很多库函数，位于"linux libc"中，libc以前曾是glibc的fork版本，但是后来glibc表现的更好，于是linux不再维护自己的libc版本，转而使用FSF的glibc。所以初步确定分析目标为glibc的某个版本。据[wiki](http://en.wikipedia.org/wiki/GNU_C_Library#A_temporary_fork)，glibc 2.x在linux中对应于soname libc.so.6，<del>于是下载glibc 2.14版本</del>。发现代码读起来还是蛮慢的，于是决定参考前人文档(见"更多资料"部分， 下文以**淘宝分析**指代)，于是下载**glibc 2.12.1**。

##man malloc

<code>man malloc</code>可以得到:  
malloc()一般从**heap**分配内存，并根据需要使用sbrk(2)调整heap size。但是当分配的内存>MMAP_THRESHOLD字节的时候，malloc()使用mmap(2)创建一个anonymous mapping。MMAP_THRESHOLD默认大小为**128KB**，不过可以通过mallopt(3)调整。

##heap size？

首先知道malloc()操作的应是虚拟内存了，故而一开始不涉及物理内存的ZONE_HIGHMEM，ZONE_NORMAL，伙伴算法之类的。事实上，真正获取物理内存的工作应是交由**缺页中断**去做了。

在文章["linux 虚拟内存， 地址空间布局， page cache， ..."](http://xanpeng.github.com/2012/03/01/buffer-cache/)中，探讨了linux中用户进程的经典虚拟内存布局，可以看到heap的位置，发现heap上面是"memory mapping segment"(mmap区域)，由此不由地想到：heap区域的大小会有多大？其增长方向是向mmap区域增长的，故而heap不能"侵入"到其中，那mmap区域应该有一个最大值之类的用以标志和heap之间的"疆界"吧。淘宝分析在2.1.1节指出:
> 这种布局是linux内核2.6.7以前的默认进程内存布局形式，mmap区域与栈区域相对增长，这意味着堆只有1GB的虚拟地址空间可以使用，继续增长就会进入 mmap 映射区域，这显然不是我们想要的。这是由于 32 模式地址空间限制造成的，所以内核引入了另一种虚拟地址空间的布局形式，将在后面介绍。但对于 64 位系统，提供了巨大的虚拟地址空间，这种布局就相当好。  

*Q：我不知道他如何得到32位机器heap只有1G虚拟地址空间的结论，如果是这样，那么32位的linux机器的malloc最大能分配多少空间呢？*

##mmap的增长方向

上面提到，在32位机器上stack向下增长，mmap区域向下增长，heap向上增长。淘宝分析2.1.3提到在64位机器上，stack向下增长，mmap区域向上增长，heap向上增长。

##内存管理概述(淘宝分析3.1.1)

（1）C风格的内存管理程序  
C风格的内存管理程序主要实现malloc()和free()函数。内存管理程序主要通过调用brk()或者mmap()进程添加额外的虚拟内存。Doug Lea malloc，ptmalloc，BSD malloc，Hoard，TCMalloc 都属于这一类内存管理程序。  
（2）池式内存管理  
内存池是一种半内存管理方法。内存池帮助某些程序进行自动内存管理，这些程序会经历一些特定的阶段，而且每个阶段中都有分配给进程的特定阶段的内存。例如，很多网络服务器进程都会分配很多针对每个连接的内存——内存的最大生存期限为当前连接的存在期。Apache使用了池式内存（pooled memory），将其连接拆分为各个阶段，每个阶段都有自己的内存池。在结束每个阶段时，会一次释放所有内存。  
（3） 引用计数  
所有共享的数据结构都有一个域来包含当前活动“引用”结构的次数。当向一个程序传递一个指向某个数据结构指针时，该程序会将引用计数增加 1。实质上，是在告诉数据结构，它正在被存储在多少个位置上。然后，当进程完成对它的使用后，该程序就会将引用计数减少 1。结束这个动作之后，它还会检查计数是否已经减到零。如果是，那么它将释放内存。  
（4）垃圾搜集  
垃圾收集（Garbage collection）是全自动地检测并移除不再使用的数据对象。垃圾收集器通常会在当可用内存减少到少于一个具体的阈值时运行。通常，它们以程序所知的可用的一组“基本”数据——栈数据、全局变量、寄存器——作为出发点。然后它们尝试去追踪通过这些数据连接到每一块数据。收集器找到的都是有用的数据；它没有找到的就是垃圾，可以被销毁并重新使用这些无用的数据。为了有效地管理内存，很多类型的垃圾收集器都需要知道数据结构内部指针的规划，所以，为了正确运行垃圾收集器，它们必须是语言本身的一部分。

淘宝分析3.1.3比较了常见的C内存管理程序，尤其指出glibc内存回收的问题，比如申请一块很大的内存，释放了其中绝大部分，仅一小部分仍被持有，则glibc必须等待这一小部分被释放。

##ptmalloc内存管理概述

linux中malloc的早期版本是由Doug Lea实现的，它有一个重要问题就是在并行处理时多个线程共享进程的内存空间，各线程可能并发请求内存，在这种情况下应该如何保证分配和回收的正确和高效。[Wolfram Gloger](http://www.malloc.de/en/)在Doug Lea的基础上改进使得glibc的malloc可以支持多线程--**ptmalloc**，在glibc-2.3.x.中已经集成了**ptmalloc2**，这就是我们平时使用的malloc，目前ptmalloc的最新版本ptmalloc3。

ptmalloc实现了malloc()，free()等函数，以提供动态内存管理的支持。分配器处在用户程序和内核之间，它响应用户的分配请求，向操作系统申请内存，然后将其返回给用户程序，为了保持高效的分配，分配器一般都会预先分配一块大于用户请求的内存，并通过某种算法管理这块内存。用户释放掉的内存也并不是立即就返回给操作系统，相反，分配器会管理这些被释放掉的空闲空间，以应对用户以后的内存分配要求。

ptmalloc在设计时折中了高效率，高空间利用率，高可用性等设计目标。在其实现代码中，隐藏着内存管理中的一些设计假设：  
（1）具有长生命周期的大内存分配使用 mmap。  
（2）特别大的内存分配总是使用 mmap。  
（3）具有短生命周期的内存分配使用brk，因为用mmap映射匿名页，当发生缺页异常时，linux内核为缺页分配一个新物理页，并将该物理页清0，一个mmap的内存块需要映射多个物理页，导致多次清0操作，很浪费系统资源，所以引入了mmap分配阈值动态调整机制，保证在必要的情况下才使用mmap分配内存。  
（4）尽量只缓存临时使用的空闲小内存块，对大内存块或是长生命周期的大内存块在释放时都直接归还给操作系统。  
（5）对空闲的小内存块只会在malloc和free的时候进行合并，free时空闲内存块可能放入pool中，不一定归还给操作系统。  
（6）收缩堆的条件是当前free的块大小加上前后能合并chunk的大小大于 64KB，并且堆顶的大小达到阈值，才有可能收缩堆，把堆最顶端的空闲内存返回给操作系统。  
（7）需要保持长期存储的程序不适合用ptmalloc来管理内存。  
（8）为了支持多线程，多个线程可以从同一个分配区（arena）中分配内存，ptmalloc假设线程A释放掉一块内存后，线程B会申请类似大小的内存，但是A释放的内存跟B需要的内存不一定完全相等，可能有一个小的误差，就需要不停地对内存块作切割和合并，这个过程中可能产生内存碎片。  

##ptmalloc2内存管理数据结构

参见淘宝分析3.2.3。  
从进程的内存布局可知，.bss段之上的这块分配给用户程序的空间被称为heap（堆）。start_brk指向heap的开始，而brk指向heap的顶部。可以使用系统调用brk()和sbrk()来增加标识heap顶部的 brk 值，从而线性的增加分配给用户的heap空间。在使malloc之前，brk的值等于start_brk，也就是说heap大小为0。ptmalloc在开始时，  
（1）若请求的空间小于mmap分配阈值（mmap threshold，默认值为 128KB）：  
主分配区会调用sbrk()增加一块大小为(128KB+chunk_size) align 4KB的空间作为 heap。非主分配区会调用mmap映射一块大小为HEAP_MAX_SIZE（32位系统上默认为1MB，64位系统上默认为64MB）的空间作为sub-heap。这就是前面所说的ptmalloc所维护的分配空间，当用户请求内存分配时，首先会在这个区域内找一块合适的chunk给用户。当用户释放了heap中的chunk时，ptmalloc又会使用fast bins和bins来组织空闲chunk，以备用户的下一次分配。若需要分配的chunk大小小于mmap分配阈值，而heap空间又不够，则此时主分配区会通过sbrk()调用来增加heap大小，非主分配区会调用mmap映射一块新的sub-heap，也就是增加top chunk的大小，每次heap增加的值都会对齐到 4KB。    
（2）当用户的请求超过mmap分配阈值，并且主分配区使用sbrk()分配失败的时候，或是非主分配区在top chunk中不能分配到需要的内存时，ptmalloc 会尝试使用mmap()直接映射一块内存到进程内存空间。使用 mmap()直接映射的chunk在释放时直接解除映射，而不再属于进程的内存空间。任何对该内存的访问都会产生段错误。而在heap中或是sub-heap中分配的空间则可能会留在进程内存空间内，还可以再次引用（当然是很危险的）。  

##如此多的内容和细节
从淘宝分析看出ptmalloc包含了如此多的内容，我的分析初衷是"徒步穿越"从malloc到物理内存分配的路线，以从代码上有个直观的认识；我的分析动力是个人兴趣。但现在看来，这项工作一时完成不了，此时尚有其他紧急任务，不得不*暂停*。

##草草的"总结"

现在知道，ptmalloc2做了很多细致的工作，以减少系统调用次数，尽量争取更少地进入内核，尽量做到让更多的用户malloc()请求在用户空间得到满足。  
（1）对于在用户态就能被满足的malloc()请求，意味着ptmalloc管理的内存已经能够满足此次用户请求，无需再从内核"批发"内存。这次分配的这些内存在真正被使用到的时候，再去做"虚拟-物理"地址映射，对应到真正的物理内存。  
（2）对于用户态满足不了的malloc()请求，ptmalloc要调用brk()去扩展内存空间，或者调用mmap()创建匿名映射，具体是哪个的细节要认真参阅*淘宝分析*和ptmalloc的源代码。  

考虑第（2）种场景。我们知道，进程并非直接操作物理内存地址，而是操作虚拟内存地址，进程的数据结构为task_struct，它包含指针指向进程的虚拟内存结构mm_struct。mm_struct就是ptmalloc"玩"的地方，如其中的start_brk，brk成员就指示着heap现在的边界，除此之外，mm_struct中还有start_code/end_code，start_data/end_data，mmap_base等。  

*Q：heap的细节和mmap区域的细节是如何表示的？*  
在文章["linux 虚拟内存， 地址空间布局， page cache， ..."](http://xanpeng.github.com/2012/03/01/buffer-cache/)的"How The Kernel Manages Your Memory"部分指出了答案，mm_struct中还有一个struct vm_area_struct结构指针，指向一个vm_area_struct列表，这个列表描述了heap和mmap区域的细节，其中heap对应于一个vm_area_struct entry，mmap可能对应多个vm_area_struct entry。  
vm_area_struct中包含起止位置值，vm_operations_struct用以指定操作函数，vm_flags用以指定权限等。

程序要使用动态内存，到这一步之后，已建立和虚拟地址空间的关系。而要真正使用物理内存时，牵涉了地址映射，缺页中断处理和物理内存分配，这是linux kernel的工作了，ptmalloc想管也管不了。

##更多问题

Q：我们知道仅在真正使用内存的时候才分配物理内存，我们也知道32位linux机器的经典虚拟地址空间布局是"用户：内核"="3G：1G"，组合考虑物理内存空间（假设物理内存为2G）和虚拟内存空间这两个因素：  
（1）如果不实际读写内存，malloc()能**一次性**分配多大的空间？  
（2）如果实际读写内存，malloc()能一次性分配多大的空间？  
64位机器上虚拟地址空间布局是"用户：内核"="128T：128T"，此时假设物理内存2G，同样考虑这两个问题。

64位linux上的代码尝试:
<script src="https://gist.github.com/2020713.js"> </script>

#更多资料
* google发现淘宝的ITer深入分析了glibc的内存管理，他的[博客](http://mqzhuang.iteye.com/blog/1064966)，他的分析文档[(pdf)](http://rdc.taobao.com/blog/cs/wp-content/plugins/glibc%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86ptmalloc%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%904.pdf)
